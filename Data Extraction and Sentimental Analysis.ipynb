{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0298524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4978ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78aa713",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613adb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page1.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1e5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title = soup.find('h1').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f99228a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rise of telemedicine and its Impact on Livelihood by 2040'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3d123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find(\"div\", class_=\"td-post-content tagdiv-type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea5a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = text.findAll('p')\n",
    "article_text = \"\\n\".join([p.get_text() for p in paragraphs]).replace('\\n', ' ')\n",
    "    \n",
    "# article_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c315c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D://Intern_FraudDetection_ML//DataSets//input_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fae3502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a2ac5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f4d11dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.3/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.4/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.8/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.2/1.5 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/e6/7c/96a44dabe8577f43ac34e34d0ac098ee42390a06fee4cbe8b5317ecf2520/regex-2023.8.8-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Downloading regex-2023.8.8-cp310-cp310-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.3/268.3 kB 17.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1decbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ba2ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1e863fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return [word.strip() for word in file.readlines()]\n",
    "    \n",
    "stopwords_files = [ \n",
    "    \"D://Intern_FraudDetection_ML//DataSets//StopWords_Currencies.txt\",\n",
    "    \"D://Intern_FraudDetection_ML//DataSets//StopWords_Geographic.txt\",\n",
    "    \"D://Intern_FraudDetection_ML//DataSets//StopWords_Names.txt\",\n",
    "    \"D://Intern_FraudDetection_ML//DataSets//StopWords_DatesandNumbers.txt\",\n",
    "    \"D://Intern_FraudDetection_ML//DataSets//StopWords_Generic.txt\",\n",
    "    \"D://Intern_FraudDetection_ML//DataSets//StopWords_GenericLong.txt\",\n",
    "    \"D://Intern_FraudDetection_ML//DataSets//StopWords_Auditor.txt\"\n",
    "]    \n",
    "\n",
    "combined_stopwords = set()\n",
    "for file_path in stopwords_files:\n",
    "    combined_stopwords.update(load_word_list(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c227a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6030dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_list = 'D://Intern_FraudDetection_ML//DataSets//positive-words.txt'\n",
    "negative_words_list = 'D://Intern_FraudDetection_ML//DataSets//negative-words.txt'\n",
    "\n",
    "positive_words = load_word_list(positive_words_list)\n",
    "negative_words = load_word_list(negative_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc5c8ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "     ---------------------------------------- 0.0/105.1 kB ? eta -:--:--\n",
      "     ----------- ---------------------------- 30.7/105.1 kB ? eta -:--:--\n",
      "     -------------------------------------  102.4/105.1 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 105.1/105.1 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/2.0 MB 2.4 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.3/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.5/2.0 MB 3.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.0/2.0 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.9/2.0 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "448f40f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to: output_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]   \n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find('h1')\n",
    "    if title:\n",
    "        article_title = title.text   \n",
    "\n",
    "    text = soup.find(\"div\", class_=\"td-post-content tagdiv-type\")    \n",
    "    if text:\n",
    "        paragraphs = text.findAll('p')\n",
    "        article_text = \"\\n\".join([p.get_text() for p in paragraphs]).replace('\\n', ' ')\n",
    "\n",
    "    if article_title and article_text:\n",
    "        output_filename = \"{}.txt\".format(url_id)\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(article_title + '\\n')\n",
    "            output_file.write(article_text)\n",
    "        \n",
    "    words = nltk.word_tokenize(article_text.lower())\n",
    "    words = [word for word in words if word not in combined_stopwords]\n",
    "    \n",
    "    positive_word_count = sum(1 for word in words if word in positive_words)\n",
    "    negative_word_count = sum(1 for word in words if word in negative_words)\n",
    "    \n",
    "    total_word_count = len(words)\n",
    "    if total_word_count > 0:\n",
    "        positive_score = positive_word_count / total_word_count\n",
    "        negative_score = negative_word_count / total_word_count\n",
    "    else:\n",
    "        positive_score = 0.0\n",
    "        negative_score = 0.0\n",
    "        \n",
    "    neutral_score = 1 - (positive_score + negative_score)    \n",
    "    polarity_score = positive_score - negative_score\n",
    "    subjectivity_score = positive_score + negative_score + neutral_score\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(article_text)\n",
    "    total_words = len(nltk.word_tokenize(article_text))\n",
    "    total_syllables = sum([textstat.syllable_count(word) for word in nltk.word_tokenize(article_text)])\n",
    "    \n",
    "    avg_sentence_length = total_words / len(sentences)\n",
    "    percentage_complex_words = (textstat.difficult_words(article_text) / total_words) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    avg_words_per_sentence = total_words / len(sentences)\n",
    "    complex_word_count = textstat.difficult_words(article_text)\n",
    "    syllables_per_word = total_syllables / total_words\n",
    "    personal_pronouns = article_text.count(\"I\") + article_text.count(\"me\") + article_text.count(\"my\") + article_text.count(\"mine\")\n",
    "    avg_word_length = total_syllables / total_words\n",
    "    \n",
    "    article_data = {\n",
    "            \"URL_ID\": url_id,\n",
    "            \"URL\": url,\n",
    "            \"POSITIVE SCORE\": positive_score,\n",
    "            \"NEGATIVE SCORE\": negative_score,\n",
    "            \"POLARITY SCORE\": polarity_score,\n",
    "            \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "            \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "            \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "            \"FOG INDEX\": fog_index,\n",
    "            \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "            \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "            \"WORD COUNT\": total_words,\n",
    "            \"SYLLABLE PER WORD\": syllables_per_word,\n",
    "            \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "            \"AVG WORD LENGTH\": avg_word_length\n",
    "        }\n",
    "    output_data.append(article_data)\n",
    "    \n",
    "output_df = pd.DataFrame(output_data)    \n",
    "\n",
    "output_file = \"output_data.csv\"\n",
    "output_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Output saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a68793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
